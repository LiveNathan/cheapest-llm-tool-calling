[Music]
Hi. So, the good thing about today is that because I already had 20 minutes in
the keynote, I think you know why I think that we should be building agents
on the JVM. So, that's good. I can move on to why you should specifically do
that using imbable rather than another framework. So this is a 100% slide uh
free presentation. So I do hope you can read the code up the back. I did check out didn't seem too bad. U but there's
going to be a whole lot of looking at my IDE. So firstly why does inbabel exist?
I got really interested in Gen AI probably two, three years ago. Well,
really interested might be an understatement maybe. If you ask my girlfriend, she might say borderline
obsessed, but it was quite a long time ago. And at that time, because I'd been doing quite a lot of machine learning
stuff, I was actually significantly more fluent in Python than in Java. So, I
started experimenting with APIs and looking at building a framework in Python.
and fairly quickly concluded that for two reasons that was not the thing to do. Firstly, Python is not ideally
suited to building I think the complex applications that we often need in business. Um and secondly that agents
are as valuable as what they can get at. What they can get at could be for
example your domain model. It could be your messaging services. It could be the investment you've made in the entities
and the way you persist them to the database. So for that reason, it was a
no-brainer that the JVM was going to be very important. So I took a long look at
the various frameworks that were out. Of course, the landscape was much much earlier um back then and broadly
speaking, I think that none of them had hit on the ideal approach. So firstly,
let's talk about the level we need. So you could say do the level of spring AI
or the level of lang chain in Python or the non-agenic part of lang chain for J.
And essentially what that's doing is valuable. It's necessary but not sufficient. What it's doing is enabling
you to call models in a portable way. That proves to be actually pretty
difficult because not only do you need to cope with the different APIs of the
different model providers. You'd need to do a bunch of magic around, you know, hopefully portable abstractions around
hyperparameters like temperature and also you need to do the tool called dance because actually while we may
think it's easy enough to just add tools using our um framework, there's quite a
lot of magic that has to go on under the covers where the LLM calls back the tool
instance and um it's all transparent to you as the developer. So that layer is
really really important and fortunately it is now a layer that is as mature in
Java as anything is when AI is concerned like you know we're looking at a whole
different level of maturity here compared to what we have been used to coming out of the JVM space. When we say
one framework is mature it's not mature like Spring um but you know it's mature
on the JVM in the context of what else is out there. However, just being able to make calls
to an LLM in a portable way is not sufficient. We also need to do
orchestration. This actually is something where I think we've seen a big recognition this week.
I don't know how many of you followed the recent um Open AI announcements around agent kit amongst other things.
Open AI are now putting a big emphasis on orchestration as part of the
solution. So they seem to have, you know, stopped talking quite so much about how AGI was just around the corner
with their models. So now they're accepting that to solve real problems, we need orchestration. We need a series
of steps and a way of wiring up those steps. So that what is what brings you to what I regard as a true agent
framework rather than a framework for portably calling LLMs. So that is not
for example in the Python world. Lang chain is the lower level thing. Lang
graph and crew AI are this kind of higher level thing that's really focused
around how you chain multiple steps together into an agentic workflow. So
looking at both what existed in Python and what existed in Java at that space I was not and continue not to be
impressed. So in the Python space there are the two most popular things crew AI
and langraph work by specifying the order of the action steps that um you
want to execute and they typically make an LLM call for each step or possibly they run some code for the individual
step. In the case of langraph you wire that up as a finite state machine. So
you specify nodes and edges. In the case of crew AI, you specify a sequence and
some elements in your sequence can kind of compose and go into their own flow.
So both of these things involve you wiring up the steps ahead of time in
code. I think those are obviously the you know the things that occur to
they're the obvious way to do it, right? Like if you thought about this problem for more than half an hour and you didn't come up with both of those
approaches as a potential approach, you're probably not a great framework developer.
But what happens if we think about the problem for a bit longer than that and we think okay what would be a good
solution that isn't just obvious and that is what I brought brought me to one
of the key differentiators of imbabel. Embable does allow you to break flows up
into multiple action steps. That's really important and you know langraph crew AI lang chain a genetic enable you
to do that and it's a really important thing to do. But in terms of how you plan the steps there are essentially
three approaches. The two existing approaches are explicit wiring up ahead
of time as in length or crew AAI using an LLM to do the planning which is
for example what Microsoft semantic kernel does or in the case of inbable we
did something quite different which is use a distinct planning step that uses a
pathfinding nonLM algorithm to work out what steps should be achieve executed in
the right order to achieve a goal. Why is this important? It's important for two reasons. Firstly, it's more flexible
than say a finite state machine. If you have a finite state machine and you want to add extra nodes and edges, there's a
bunch of wiring and rewiring you need to do, right? If you're addressing a problem like regular expression parsing
where regular expressions don't change very often, that's fine. But if you're dealing with business logic where it
changes a lot, that's not necessarily ideal. Secondly,
when you're ordering actions separate from the definition of action,
but like ordering the actions explicitly in code, you lose type safety. For
example, in Crew AI, when you specify the order of your tasks, as they call
actions, well, you could be wrong. like the next task is relying on the magic um
thing that apparently went into the map in the previous task with the right string key. So if you're doing that kind
of ordering unless you're like directly invoking it in code like do thing A do
thing B if you're doing that kind of ordering with a framework you're throwing away typing. So that brought us
to the core innovation in in Babel which is we do planning but it's deterministic
and it's not done in your code. Right? Let's let's make this more concrete by code examples. So firstly I'm going to
show how easy it is to get started with imbable. To do that you go to the Java
agent template. So if you go to the um Java agent template at inbable you will
see that you've got this lovely little use this template button. Um this is some nice functionality GitHub has. You
can create your own repository in your own organization very very quickly. If you have an open AI or enthropic key you
will have your agent up and running um in under a minute. So what I'm going to
do is look at what's in that Java agent template. Standard Spring Boot application. In this case, we're using
Maven, but of course you can use Gradle. Notice we extend the Spring Boot parent
and we bring in the imbable starters that we want. So in this case, we bring
in the core imbable agent starter and we also bring in the starter for OpenAI
models. You could use starter anthropic starter bedrock. There was a workshop on
uh Monday that um covered the support we have for bedrock. So all of those things
are brought in as extra starters. So very familiar to Spring Boot. So then of
course you would need a application main class. Again we're very much in the
Spring Boot world here. Um and you just define a spring boot application and you
add a single thing to it which is at enable agents. So add enable agents means that imbabel
is now going to participate in the class path scanning that spring does and it's going to identify agents and actions
that will be useful to it. Another important thing here is the logging
theme. All logging in in imbable is done in response to events. Uh this is
something I always wanted to do with spring but it was a bit too late to retrofit. So all info level logging
comes from reacting to an event. And this is a really good discipline because it means we have to emit the correct
events so that you understand exactly what's happening. Those events can also obviously go over the wire with SSE or
alternatively you can write application code that reacts to specific events. So
because logging is done um in response to events, we can easily change the
style of our logging. So we have the notion of logging themes and we've got a few out of the box including Star Wars
and Severance for fans of that show and the default one for this is Star Wars.
My experience is that most people in the community think this is great and fun, but a very small number of people really
really disapprove of it. And the answer is that. And now you get perfectly normal boring logging. But I'm going to
put it back. Okay. So this is the um
Java agent template. This is what you would get if you ran it yourself. So you
can start it up with a shell or in the IDE. And by default, you can see we've
got our Star Wars logging here. Um, and he's in one of the um, approved colors.
I think it might have been Tatooine blue. Um, someone in the team looked it up. Um, so by default, it starts up in
Spring Shell. It's a really nice interactive way of adding functionality because you can add your own at shell
components with at shell methods. So you know obviously this is not how people run inbabel in production but it is very
very useful to get started. So we now have an imbabel server running and a
different thing about say inbable to take spring aai is inbabel really is a
server so it knows everything that's deployed on it. It's able to manage processes. So when it's running agent
processes they might be long running. They might even wait for human response. So we've got a server here. Let's ask it
a bit about itself and we have some shell commands for that. So let's firstly ask it what models it has. So
here it tells us that it's got access to various open AI models. This is because
we brought in the open AI starter. We also support any model that's supported by Spring AI. So for example, bedrock
models, we did a whole workshop on that. Any bedrock model you can get access to. Enthropic models, the claude models are
severely rate limited, but they're very very good. Um, so clawed models, if you have your anthropic key and bring in the
right starter and also lama models,
local models are becoming more and more important. I mean if you look at the
pace of improvement of the large commercial models like for closed source
models like say GPT um or claude I would say the rate of improvement is slowing if you look at
what's happening with local models so the smaller open models it's accelerating so the best local models
are now amazingly good and one of the benefits of using any true agent
framework like in Babel or Crew or Langraph is you're breaking flows down
into smaller steps. Those smaller steps are more amendable to running on a local
model. So we support OAMA um local models. We also support the Docker um
local model capability and you have a wide choice of models that you can run at varying sizes and I strongly
encourage you to look at that. But this one's going to run against OpenAI. Now
let's ask it about agents. So there's a bit of um stuff here that I'll explain
later, but at the moment the TLDDR is it's got a write and review agent. Generate a story based on user input and
review it. So this is a simple agent that consists of two steps in the workflow. The first one is it generates
a story using one LLM call. The second one uses a different LLM call to review
that story. So fairly common um kind of pattern, pretty simple but um
nevertheless somewhat interesting. But let's ask
a little bit more. Let's ask what goals we have. So here you can see the goal is
the story has been crafted and reviewed by a book reviewer and the goal has a
set of preconditions. Let's talk about how Inbabel does action planning. I said you don't need to wire it up explicitly.
What you do is you write your code and in Babel works out what things need to be run in what order to achieve the
appropriate goal. So actions have preconditions
and expected postcond conditions. Goals have only preconditions.
So these are assessed from the world state. This is an algorithm called GOP,
goal oriented action planning. It is one of the most popular algorithms used in
controlling AI characters in gaming. So I like to say that although hardly any
enterprise Java developer has heard of this algorithm, I think you've nearly all been killed at some point by a gulp
AI. It's really a manif a repackaging of a
very very old AI concept. So back in the 50s there was something called general
problem solver that was fairly similar to this. In the 70s there was something called strips. Um and it's this concept
of essentially a pathf finding algorithm like aar through a series of world
states. Why is this good? Well firstly it means that your system will always choose the
same path with given world state. Right? So, it's not like an LLM where it can't
explain why it chose to do something. It can explain exactly why it chose the steps in what order.
Um, and it's also, you know, really pretty fast compared to an LLM. But
because we're planning with the actions and goals we know about,
we can add more actions and goals and then our system potentially can do more
things. So because you're not needing to wire things together explicitly, say with a finite state machine, you can
increase the power of your system without making it non-deterministic by adding more actions and goals. Okay, so
let's see a simple example of this. Wow, that's okay. We're good. So we have one
um agent which is to craft a story. So, I'm just going to invoke it saying, tell me a story about what shall we um tell a
story about? Loudest shout wins. Star Wars. Star Wars. Okay. Yeah.
So, this with this mode of invocation, what it will do is choose a goal using an LLM and obviously the write and
review story goal is appropriate here and then it will execute it. In a distant galaxy, young Kira discovered an
ancient holocron buried beneath the sands of Tatooine. Okay, so we've got
our um we've got our story and we've got our review. The story is a little bit
it's a little bit wacky like empowered Kira ignited her lightsaber ready to
challenge. It's it's expressed in kind of more flowery language than the review. Let's see why that is. So now
I'm going to run it again with the -ashp option so you can see exactly what
prompts it emitted. Well, actually we'll look at a little bit of code first and then we'll get
back to that. So what do we have here in our write and review agent? How did know
that it existed? The way it knew it existed is that
it saw a class with at agent annotation on it. ad agent is a spring meta
annotation. So this means that just the same way for example a rest controller
is a spring bean. Um any imbable at agent is a spring bean. So it's going to
get injected. It's going to get you know security whatever services the spring container has to offer. In this case, um
the um constructor does take a couple of parameters which are um defaulted. Um so
it uses spring value parameters but you know in real more sophisticated agents you will inject other services that come
from your spring JVM world. So think about that for example let's suppose you
were um doing this with crew AI in Python not going to be injecting Java services
getting at the Java domain objects that probably represent a lot more of your business value in your organization than
anything you have in Python. So you know firstly we are as close to the spring world with this as if we were writing a
rest controller. Secondly let's look at where these actions come from. So if we
ask it before we run it again with the prompt, let's ask it what actions it knows about. We ask what goals. Let's
see what actions. So review story and craft story and it's got the planner
information about them. But here for example is the craft story action.
The craft story action takes two parameters. One is user input which is a
like standard framework class that's useful for initial binding use initial user input. The second one is operation
context. Operation context is a special thing that enables you to get at the
capabilities of the platform like calling models. Um but
the way in which action preconditions and goal preconditions are determined in
imbeable is around data typing. I believe it is by far the most type of
any framework. Um, and you never never use rely on string keys. When you build
your prompts, it's type safe. When you look at the data flow between different actions, it's type safe. So, what this
says here is that this action has a precondition of user input, right? You
can't do it unless you've got user input. Um, and it's got an expected post
condition of a story object. as your pro agent process executes it has a
blackboard. So it has it's maintaining state.
So user input um is a domain object in this case. Operation context is not
operation context and AI are just imbalable um types that are really handy
to use but don't affect your control flow. Secondly, we've got this other
action which is annotated without achieves goal. So this is an action that
takes user input and a story and returns a
reviewed story. The achieves goal annotation is a convenient way of saying
when you've executed this action, you've achieved the following goal. So it's just it's just really a shorthand. So
this defines both an action and a goal. So the planner can look at your code and
it can work out what order these operations need to run in. So it's completely type safe. You're not
specifying ordering of things that could potentially conflict with the types. It will work out that it can run them in
that order. If you wanted to add any custom preconditions, you could also do that by saying which particular
preconditions you want. But most of the time your conditions are satisfied by
data types. So if you look here at the more detailed output, you can see for
example the precondition of craft story um is that it hasn't run um and we have
user input. So let's run it again with um more
explicit prompt. Firstly, we'll just take a quick look at how the craft story
method works. So, from the context, we get the AI
object. You can often inject in um embable the AI object or the operation
context. Usually, if you've got a choice between the two, it's better to do the AI object because all that does is
enable you to run prompts. And it's good practice to, you know, use the least powerful thing you can. But in this
case, we're getting AI from operation context. We now go into our fluent
interface um that makes it really easy to custom choose LLMs, customize the
calls and run prompts. So we're saying with LLM and we're saying auto LLM with
temperature, we could also if we wanted to do with max tokens. All of this is
very very easy, very toable interface. We're adding here a prompt contributor.
So, Embable helps you structure prompts from reusable piece parts. They can be
packaged as the prompt contributor interface. You don't have to do this, but it can be quite um handy. So in this
case, this structures um the information about the persona that you want in um
the writer. Um and then we call one of the um
methods to either generate text or create an object that you have on prompt
runner. So in general, you want to create objects. Just because your LLM
speaks natural language doesn't mean that you should always talk to it in natural language. It doesn't mean you should ask it to return purely natural
language. So generally you want to return as much structure as you possibly
can. What we're saying here is given this prompt
return create an object of story class. As you can see here, the prompt is
created in code. We have support for externalizing
prompts into Ginger templates, and that works really quite well when you've got big fat complicated prompts. But my
experience is that if you're breaking things up correctly into multiple steps,
you're better to build your prompts in code. Prompt engineering. Once you break
things down into small prompts with fewer tools, prompt engineering becomes
less of a thing and prompts in code make a lot of sense. Prompts in annotations
like lang chain forj I think make no sense at all. But prompts in actual code where you can get it all the objects in
scope make a lot of sense. So this is generally the way you do it. And then of
course you can do anything you like with the uh what you get back. Right. Let's now run it again and we'll ask it to
show prompts. Tell me a story. Actually say story about ant. I'm curious what it's going to do. So I'm asking it to
show prompts. So here we get a lot more um detail
and let's look at the prompts and the planning information. So at the very
beginning Yoda said he created a process create the process I have and is
planning from this world state. So the world state is we don't have a story
object we don't have a review but we do have a user input. So the planner is
able to work out this plan towards the goal. So the goal is to have a reviewed story. What we can do is craft the story
call review story and then we will meet that goal. One of the other advantages
of this dynamic planning approach is
it can reroute after every execution of an action.
The planner reassesses world state and replans. In most cases, it's just going to find that the plan is working.
Everything um is going very well. But imagine if you didn't achieve the
expected effects from a particular action. There might be another way to get to the goal. An example of this is
for is say you have insufficient input, but you could ask the user for another piece of information. It's not something
you want to do by choice because you would love not to throw a form up, but you know the planner in one of our
examples can um get to the point where it says, "Okay, well, I'll throw up a form because that way I can get what I
need. So it then started executing these actions. So you can see it executed the
first action. It's using GPT41 mini. It's using GPT41 mini because we said
with auto LLM. So it's using the LLM that we've marked as the default on this
instance. Of course we can change that. Um and here is the prompt. It's got a
little bit more into it than what we wrote. Like obviously it's got these things here. here the craft a short
story that we wrote but it's also got the persona information remember I
brought in that prompt contributor and it's also got the current system date
and or current date and the knowledge cutoff of the model that's actually really important the framework within
Babel puts that in by default particularly if you're getting into tool calling if the model doesn't know its
knowledge cutoff date and the current date it doesn't know how to search properly Okay. So, a few months ago, um
before I put that functionality in, I was asking through, um an agent about
the recent Australian general election. Even though the agent had access to web
tools, it was happily telling me about the 2022 election because thought that was recent. I guess it kind of makes
sense. It's like it it interpreted to be the most recent Australian election. it knew about. As soon as we added that
functionality where it puts in the current date and the knowledge cutoff, it will use tools correctly. You might
be thinking here, how do we know the current date? Well, that's pretty easy. But the tricky bit is how do we know the
knowledge cutoff of the model? And with all the models that we support out of the box, we actually assign their
knowledge, we include their knowledge cutoff and also their cost per token, which is also important. Um, but we're
also building a larger LLM database because there are a bunch of things that you really really need to know to use
models usefully. So, we're probably going to unveil our LLM database in the next month.
I mentioned that we know about the knowledge cutoff. We also know about the
models pricing. Well, we may or may not know for all the popular models we know.
So when inbabel executes say a multi-step flow across many models, it
tracks the token usage, input and output token usage because they're um typically
um priced differently. Um, it actually uses Spring AI to do that, but we add this extra thing where we compute the
price of all those input and output tokens totaled across all the models.
This is actually another really cool thing about this is because on the agent process as it's running this usage and
pricing data is exposed, you can actually write agents that are aware of
their cost. So for example where you can expose a tool that returns the cost and
can for example suggest that the um LLM should use fewer tokens or call fewer
tools. So very powerful um capability. So it ran the first um prompt and
there's also additional stuff that isn't shown in this log to do with the type marshalling. So under the covers, Spring
AI went to looked at the um story class that was
supposed to be returned and generated a JSON schema and gave it to the LLM and said you must return this thing.
Then after that we executed the um second prompt different persona here um
so different prompt contributor also different hyperparameters. So when we did the review story um you can see that
we didn't bother setting the temperature. The reason you know the first output's a little bit flowery is
because we set the temperature to 0.7 which is very high. Okay. Now let's look
at a slightly more um complex example. We're going to look at something that
uses tools. So in the in real agents, tools are vitally important. Tools are
what enables the LLM to reach back into its environment, right? MCP tools are
very very important. But if you're working in the JVM,
tools that are exposed from your Java code are incredibly important. So you can just, for example, put an at tool
annotation on any method and you can expose it to LLMs. And in Babel makes
that particularly easy. That is a killer capability when you want to expose
existing business functionality. So this one is our imbable agents examples um
repo and it's got a lot more things in it. So for example, it's got a fact
checker. It's got a book writer which was um taken from a crew AI example and
I think is much much nicer in Java within Babel. and it's got the star newsfinder which is what we're going to
show now. So what this does is given a person and their star sign
it looks up their horoscope. Looks up their horoscope using REST client. It's
just an API call. It's not an LLM isn't doing that. Then it runs another LLM
call using web tools to search the web for news that seems to be relevant to the horoscope today. And then finally it
puts it all together into a write up. So let's try this.
And we're going to show prompts here. So this has got more moving parts um than
the previous one. There's going to be a longer set of actions. So you can see
where's its plan? So here's the plan. So, the plan is extract star person,
retrieve horoscope, find news stories, and write up. Ooh, the tools are This
isn't really going to work because the tools don't seem Oh, saying tools are not enabled. Maybe there's something
wrong with my Docker configuration. That's that's unfortunate though. Um, so
it's possible that these aren't true results from the web. Um, but that's purely a misconfiguration rather than
anything in in Babel. Okay. So, at the end of it, we get a pretty wacky story.
Scorpio special, chaos is your friend today. Dear Linda, the stars are basically saying, throw your plans out
the window and embrace the weird today. Let's see how this particular sausage
was made. So, if we go to the star news, by the way, we've got an example of this
in Cotlin as well. The framework itself is largely written in Cotlin. Um, but
most of our users are on Java and we've put a lot of effort into making sure the API is beautiful from Java. Okay. So,
pretty much the same kind of thing like we start off with um a class, we inject
that class using spring. So, for example, where did our horoscope service come from? It's just a spring um service
and it's implemented using restclient. The first step here was an add action
that's extract star person. So this given user input um try it calls create
object if possible. This is something that we add to what spring AAI can do.
So whereas Spring AI can return you an object of an individual type, we have
the ability to say if you can create it, but if you don't, say why you couldn't do it. Um, and return null. That
actually tends to be quite powerful because this is one of the examples where if we can't do that, we could
reroute. We could ask the user. For example, if we didn't get a star sign um or we didn't get a name, we could go and
ask the user. So this idea of you know create object if possible um is I think
quite um important. Everything that you do ultimately in running prompts within
Babel will go through the context and AI interface. This has quite a lot of
benefits. For a start it's a really really nice API that we've put a lot of effort into. Um, and secondly, it means
that in Babel agents are really easy to unit test because it's very easy to mock
just that one interface. Even with integration testing, you can easily use a mock um chat model and actually um see
your flow run with the kind of, you know, successive results that you want to see. So, you know, there's a lot of
effort on putting a really convenient nice to use API. So now let's assume it's got a star person. It's able to run
this retrieve horoscope method. Again, any of these if we want to unit test them, it's like it's a POJO, right? It's
very very easy to unit test them. We don't have to worry about whatever magic fields might be in any map. We can just
instantiate the object and call the method. So here as you can see actions
can either call LLMs or actions can
simply call code. Actions that call code are absolutely awesome. Anything that
you can do purely in terms of code rather than invoking an LLM, you should
absolutely do in code. It's way faster. like, you know, essentially almost
anything you can do in code compared to anything you can do with an LLM is instant. Um, it's way faster, it's way
cheaper, it's more reliable, and it also, you know, if you care about the planet, it's going to use a lot less
water and electricity. So, one of the major benefits of any agent framework is
break your flows up. So, you have individual LLM calls and you have
structure all along the way. And this makes it much easier to mix in code
interactions. So here the next thing once we've got our star person, our horoscope um and
our horoscope, now we can build a prompt to call web tools. So notice here we
specified tool groups. So in Babel gives you um two different ways of getting at
tools. One is the notion of a tool group. So we like to introduce a degree of interaction. You don't ask for
example for duckgo or Google or brave search. You ask for web search the web
search tool group. This means that in different environments you can easily um
change the underpinnings. It also mean the services that you use. It also means
for example that you could write something that is aware of user access levels. So for example, you know, maybe
you get the super fast search um if you're on a high tier. So I think there's a lot of benefits in adding that
level of indirection. So now because we've added the tool groups at action
level, they will be added to the LLM. You also um can um add tools like this
if you want. So for example with tools. So you can specify tool
groups or you can do with tool object and when you specify a tool object it
will expose all the at tool methods that are on that instance. That is super
powerful. One of the best places to use that is where you're working with
entities. So there's a couple of our examples that do this. So the first step of the action it's not using an LLM.
First step of the action is for example looking up a customer because the request included a customer ID. Then it
exposes that customer to the prompt. So any tools that are on the customer can
be invoked. Really powerful pattern because amongst other things it means that you can keep data away from your
LLM that shouldn't go to your LLM. Right? like the tools on the customer
will act will act on customer state but you probably actually don't want to send over the wire to open AI or anthropic
and particularly in Europe um obviously that's a major issue
if you use this pattern that sensitive data stays in the entity backing the
tool and the LLM never knows about it so tools are very very important
um so and at the And we we call we put all of these things together.
So I'd like to move on now to a fancier example, which is our travel planner.
When we restart this, we're going to see whether or not I get any warnings. If I get warnings, my Docker configuration
may be messed up. I think this one's going to work. So this is actually connecting to the Docker local MCP
gateway. Um, this is quite powerful and an easy way to get started with MCP.
Like obviously one thing you can do with MCP is run a bunch of tools as NPX or
UVX or whatever they're exposed as. Really, really messy. The Docker MCP
gateway is quite nice because it puts it in a consistent place and can handle secrets for you. So, this is our
fanciest example. Um, and this is a travel planner.
put got a simple HTMX web interface on it and it's a fairly complicated flow.
So I'm going to kick it off and then start talking through the code while it runs. This is going to take this going to take a few minutes to run. So this is
completely live by the way. The like I'm not going to change this because I know to talk about the defaults. Um but it
totally works and you can get it running yourself um fairly easily. Just do take
care because an average run will cost you 10 to 20 cents. But on the other hand, it's a really really good travel
planner. So let's assume that um Claude and Ingred want to go from Barcelona to
Bordeaux in the next couple of weeks. We know that Ingred loves history and museums and Jonah Arc and Claude loves
food and wine, which is probably why he wants to go to Bordeaux. Well, it says he has a particular interest in Cabernet, so that's certainly why he
wants to go to Bordeaux. Okay, so I've kicked it off. You can see that there's
um some events coming out here. These events are from the same event stream
that we used for logging. So let's have a look at the implementation of the
code. This one is actually in Cotlin. So the very first thing you should do when
you need to write an agent is create a domain model. either find an
existing domain model or create a domain model. Create think about your domain objects before you think about your
agents. Trust me, your agents are easy to write once you have the domain
objects. So in this case, we start with a journey travel brief where we want to
go from, where we want to go to, how we want to travel, and the dates. Notice we got a little bit of structure in this
already because the dates are date objects um not just strings. We have travelers travelers not very
interesting name and a little bit about them. It corresponds to what we have on that form. So what our flow does is
ultimately comes up with a um proposed travel plan. So you can see here that
I've added a bunch of JSON property descriptions. these will be sent to the
model in the JSON schema. Um, so it's a way to hint a little bit more about what
those fields mean. To be honest, LLMs are pretty smart. So most of the time if
you use sensible um field naming, it will just work. But in this case, we've just gone the extra mile. So the step
the flow in the agent is let me go back to the tripper
agent tripper agent. So this repo it's tripper t r i pp er um just it in babel.
So the agent is configured its configuration is using spring at
configuration properties and actually we have this in a yl um file is a bit more
structure in it. So you know anything that you are used to doing with spring you can just do.
So the first action that will run is find points of interest. So in this case
we use the thinker llm. Um this is specified in application.yml.
So if we look at thinker llm and it's using gpt41.
So it's using a pretty good model because we want to look at the um the
journey that people are making and deciding decide what is interesting in
that context. Okay. So let's go back to
here. So using that LLM we add the prompt elements of um the travel
planners persona and the travelers and we give it a number of tools here. We
give it um web tools, we give it map tools and we give it map math tools. So
it's going to be able to use in this case Google maps to um work out what are
interesting places. it returns this itinerary ideas type. So very often you
will use your existing domain objects but you also create these little you know interim objects that are useful for
LLM return structures really important thankfully even in Java now we have
records which are pretty much equivalent to cotlin data classes so it's a very concise thing to do I think this may
have finished let's have a look um yes it has finished epic autumn road from
gothic Barcelona owner to Bordeaux wine legend. So we have here an interactive
map which I'm very confident will work. The reason I'm very confident in that
link is that it wasn't built by an LLM. When I initially wrote the first version
of this, I let the LLM generate the Google Map URL.
It looked pretty good, but the thing's about 300 characters long. You just need one funny character in there that it
hasn't escaped and you're totally messed up. So, let me show you how I solved that because it's a very good
illustration of why we take this overall approach of breaking things down. So
here we have the journey map URL
property and it's actually computed because when we asked the LLM to create
the travel plan, we didn't just say give us text. We said give us a list of where
we'll be on particular dates. Because we've got that structure, we can write
code that works against it. So this is what we asked the LLM to return to us.
So this was in originally inspired by a crew AI example that naturally just asked for a string. You can't do any
interesting post-processing on a string. So for example, imagine here if well
you've already seen that we can build the map URL this way. We can also compute how long we stay in each place
because if we see that days we have two days in the same location will be probably staying in the same
accommodation. So it's very useful for example when we go and talk to the Airbnb API. Similarly we've got a list
of countries visited which isn't just text. So for example let's suppose we
have visa concerns. So let's suppose we want to know for example for me as a British and Australian dual citizen do I
need visas um for some of the places I'm going. So the more structure you put the
better guard rails you can have the more validation you can do and the more of your behaviors you can move to code. Um
so finally after we um did that so let's briefly go through these steps in the
agent and right so we found points of interest then in the second one we
researched points of interest so we you we create a prompt runner and we use
parallel map on the operation context to run research with many LLM calls in
parallel we're always careful to bound the number of calls we take in parallel because if we don't do that we'll get
rate limited or worse. But as we work through this we were able to get to the
structure um and we were able to go down to actually
use the Airbnb API to find availability for the correct dates. We're able to put
in useful links and also we were able to log the usage. It cost 19 used two
different LLMs. It used GPT41 for the tricky things and it used GPT41 mini, a
cheaper model for doing the research because the research was pretty easy. Okay, I think I'm out of time, but
hopefully that wetted your appetite. I do believe this is not just the best way
of building agents on Java. I think it is the best model period. And you know, we're not aiming to play catch-up with
Python. We're aiming to prove that Java and the JVM is the best place to build
agents. Thank you.
[Music]
[Music]